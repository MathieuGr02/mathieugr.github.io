{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "title: Statistical Learning Theory\n",
    "layout: collection\n",
    "permalink: /Machine-Learning/Statistical-Learning-Theory\n",
    "collection: Machine-Learning\n",
    "entries_layout: grid\n",
    "mathjax: true\n",
    "toc: true\n",
    "categories:\n",
    "  - study\n",
    "tags:\n",
    "  - mathematics\n",
    "  - statistics\n",
    "  - machine-learning \n",
    "---"
   ],
   "id": "1803b6d0f8e72066"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Frequentist Decision theory\n",
    "\n",
    "Let $\\mathbf{x}$ be an i.i.d generated sample from an unknown pdf $\\mathbb{P}^\\*(\\mathbf{x})$ and the output values $y$ be from the unknown conditional pdf $\\mathbb{P}^\\*(y | \\mathbf{x})$.\n",
    "The language model is then trained on a set of observed pairs drawn from the true unknown pdf given by $\\mathbb{P}^\\*(\\mathbf{x}, y)$.\n",
    "With this we get our data $\\mathcal{D}=\\{(\\mathbf{x}_1, y_1),...\\} \\overset{i.i.d}{\\sim} \\mathbb{P}^\\*(\\mathbf{x}, y)$. \n",
    "We assume here that $\\mathbf{x}$ is observable, but $y$ isn't. \n",
    "We then consider an estimator $\\delta(\\mathcal{D})$, which is defined as a prediction function $\\hat{y} = f\\_{\\mathcal{D}}(\\mathbf{x})$.\n",
    "Since the data is random, so is the estimator, meaning the estimator is a RV. \n",
    "We can thus compare the estimates class membership with the true label via a loss function $L(y, f(\\mathbf{x})$.\n",
    "The expected risk is then the average loss with respect to the true unknown joint distribution $\\mathbb{P}^*(\\mathbf{x}, y)$, or it can be thought of as expectable error. \n",
    "\n",
    "$$\n",
    "R(f, \\mathbb{P}^*) \n",
    "= \n",
    "\\mathbb{E}_{\\mathbb{P}^*(\\mathbf{x}, y)}L(y, f(\\mathbf{x}) \n",
    "= \n",
    "\\mathbb{E}_{\\mathbb{P}^*(\\mathbf{x})} \\mathbb{E}_{\\mathbb{P}^*(y | \\mathbf{x})} L(y, f(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Because the expected risk itself depends on the unknown distribution $\\mathbb{P}^*$, and we don't know the true nature of $\\mathbb{P}^*$ we cannot calculate this risk directly. We can approximate it with the so called emperical distribution for the n observed samples in $\\mathcal{D}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}_{\\mathcal{D}}(\\mathbf{x}, y | \\mathcal{D}) = \\frac{1}{n} \\sum_{(\\mathbf{x}_i, y_i) \\in \\mathcal{D}} \\delta({\\mathbf{x} - \\mathbf{x}_i) \\delta(y,y_i)\n",
    "$$\n",
    "\n",
    "The emperical risk, which is the sample average of the loss is then given by \n",
    "\n",
    "$$\n",
    "R_{emp}(f | \\mathcal{D}) = \\frac{1}{n} \\sum_{i=1}^n L(y_i, f(\\mathbf{x}_i))\n",
    "$$"
   ],
   "id": "7e2c4c3df2cee458"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hypothesis space\n",
    "\n",
    "The best possible risk is defined as $\\text{inf}_f R[f]$. Because we often have to restrict our function space, thus our hypothesis space $\\mathcal{H}$, which is only a subset of the true space. \n",
    "In our given hypothesis space we define $f^*$ as the best possible function that can be implemented by a machine.\n",
    "\n",
    "$$\n",
    "f^* = \\arg \\min_{f \\in  \\mathcal{H}}\n",
    "$$\n",
    "\n",
    "We also denote $f_{\\mathcal{D}} \\in \\mathcal{H}$ as the empirical risk minimizer on a sample $\\mathcal{D}$\n",
    "\n",
    "$$\n",
    "f_n = f_{\\mathcal{D}} = \\arg \\min_{f \\in \\mathcal{H}} R_{emp}(f | \\mathcal{D})\n",
    "$$"
   ],
   "id": "73020ff43ccce45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Convergence\n",
    "\n",
    "SLT gives then results for bounding the error on the unseen data, given the training data. Thus a relation between the past ($\\mathcal{D}$) and the future (unseen data). In SLT all samples are i.i.d. We can bound the risk which with a probability of $1 - \\delta$ holds with $a, b > 0$:\n",
    "\n",
    "$$\n",
    "\n",
    "R[f_n] \\leq R_{emp} + \\sqrt{\\frac{a}{n}\\left( \\text{capacity}(\\mathcal{H}) + \\ln \\frac{b}{\\delta} \\right)}\n",
    "\n",
    "$$\n",
    "\n",
    "Ont he covnerge of RV themselves, we say that X_n converges in probability to the random variable X as $n \\rightarrow \\infty$, iff, for all $\\epsilon > 0$, \n",
    "\n",
    "$$\n",
    "\\mathbb{P}(|X_n - X| > \\epsilon) \\rightarrow 0, \\text{as } n \\rightarrow \\infty\n",
    "$$\n",
    "\n",
    "We define this as $X_n \\overset{p}{\\rightarrow} X$ as $n \\rightarrow \\infty$."
   ],
   "id": "8f22fef979486751"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Binary classification\n",
    "\n",
    "In general here we only look at the binary case where $f : X \\rightarrow \\{-1, 1\\}$ with $L(y, f(\\mathbf{x}) = \\frac{1}{2}|1 - f(\\mathbf{x})y|$. \n",
    "The hypothisis space is then $\\mathcal{H}' = \\{f' = \\text{sign}(f) | f \\in \\mathcal{H}\\}$"
   ],
   "id": "7636ceda9404b107"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Consistency of Emperical risk minimizations (ERM)\n",
    "\n",
    "The principle of ERM is consistent if for any $\\epsilon > 0$, \n",
    "\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|R[f_n] - R[f^*]| > \\epsilon) = 0\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|R_{emp}[f_n] - R[f_n]| > \\epsilon) = 0\n",
    "$$\n",
    "\n",
    "Meaning that for $n \\rightarrow \\infty$ for the first case, the probability fo the true risk $R[f_n]$ deviating from the best possbile risk $R[f^*]$ in our hypothisis space becomes zero, which means that the true risk of $f_n$ converges to the risk of the best possible function. \n",
    "The seconds case describes that the emperical risk, which is estimated from the data converges to it's true risk. \n",
    "\n",
    "![consistency erm](consistency_ERM.png)\n",
    "\n",
    "Only the condition $\\mathbb{P}(|R_{emp}[f] - R[f^*]|)$ does not suffice as a condition for consistency."
   ],
   "id": "730dace92a05e44b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hoeffding's inequality\n",
    "\n",
    "With Hoeffding's inequality we are able to bound our probability.\n",
    "Let $\\xi_i , i \\in [0, n]$ be independent instances of a bounded RV $\\xi$, with values in $[a, b]$. Denote their average by $ Q_n = \\frac{1}{n} \\sum_{i} \\xi_i $. Then for any $ \\epsilon > 0 $ we get:\n",
    "\n",
    "$$\n",
    "\\begin{rcases}\n",
    "\\mathbb{P}(Q_n - \\mathbb{E}(\\xi) \\geq \\epsilon) \\\\\n",
    "\\mathbb{P}(\\mathbb{E}(\\xi) - Q_n \\geq \\epsilon)\n",
    "\\end{rcases}\n",
    "\\leq \\exp \\left( -\\frac{2n\\epsilon^2}{(b - a)^2} \\right)\n",
    "$$  \n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\mathbb{P}(|Q_n - \\mathbb{E}(\\xi)| \\geq \\epsilon) \\leq 2 \\exp \\left( - \\frac{2n\\epsilon^2}{(b - a)^2} \\right)\n",
    "$$\n",
    "\n",
    "Looking at our binary classification, we define $\\xi$ to be a 0/1 loss function\n",
    "\n",
    "$$\n",
    "\\xi = \\frac{1}{2}|1 - f(\\mathbf{x})y| = L(y, f(\\mathbf{x}))\n",
    "$$\n",
    "\n",
    "Then we get that $ Q_n[f] = \\frac{1}{n} \\sum_{i=1}^{n} \\xi_i = \\frac{1}{n} \\sum_{i=1}^{n} L(y_i, f(\\mathbf{x}_i)) = R_{emp}[f] $ and $ \\mathbb{E}[\\xi] = \\mathbb{E}[L(y, f(\\mathbf{x}))] = R[f] $.\n",
    "becuase $\\xi_i$ are independent instances of a bounden RV $\\xi$ with values in [0, 1] we get\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(|R_{emp}[f] - R[f]| > \\epsilon) \\leq 2 \\exp \\left( -2n\\epsilon^2 \\right)\n",
    "$$\n",
    "\n",
    "With this, the hoeffding's inequality gives us a rate of convergence for any fixed function. \n",
    "This doesn't tell us that $ \\mathbb{P}(|R_{emp}[f_n] - R[f_n]| > \\epsilon) \\leq 2\\exp \\left(- 2n\\epsilon^2 \\right) $, because $f_n$ is not a fixed function. $f_n$ depends on the data $ \\mathcal{D} $. \n",
    "Because $f_n$ is chosen to minimize the emperical risk, it may change with inceasing n so it is not a fixed function, therefor the hoeffding's inequality cannot be applied to this convergence.\n",
    "For each fixed function $f$, we get $R_{emp}[f] \\xrightarrow[n \\rightarrow \\infty]{P} R[f]$, meaning that the emperical risk converges to the true expected risk for a function as $n \\rightarrow \\infty$ (point wise convergence).\n",
    "\n",
    "# Conditions for consistency\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f_n &:= \\arg \\min_{f \\in \\mathcal{H}} R_{emp}[f] \\\\\n",
    "    f^* &:= \\arg \\min_{f \\in \\mathcal{H}} R[f] \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\begin{align*}\n",
    "    R[f] - R[f^*] &\\geq 0, \\ \\forall f \\in \\mathcal{H} \\\\\n",
    "    R_{emp}[f] - R_{emp}[f_n] &\\geq 0, \\ \\forall f \\in \\mathcal{H}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For first case, because $f^*$ is the function that minimizes the expected risk for all functions in the hypothisis space, any function in the hypothisis space has an equal or higher risk compared to $f^*$. \n",
    "For the second case, because $f_n$ is the function that minimizes the emperical risk for all functions in the hypothisis space, any function in the hypothisis space has an equal or higher emperical risk compared to $f_n$. \n",
    "Because this holds for any function $f \\in \\mathcal{H}$, we set $f = f_n$ for the first case and $f = f^*$ for the second one.\n",
    "$$\n",
    "\\begin{align*}\n",
    "    R[f_n] - R[f^*] &\\geq 0 \\\\\n",
    "    R_{emp}[f^*] - R_{emp}[f_n] &\\geq 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can then write\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    0 \n",
    "    &\\leq \n",
    "    R[f_n] - R[f^*] + R_{emp}[f^*] - R_{emp}[f_n] \\\\\n",
    "    &=\n",
    "    R[f_n] - R_{emp}[f_n] + R_{emp}[f^*] - R[f^*] \\\\\n",
    "    & \\leq\n",
    "    \\sup_{f \\in \\mathcal{H}}(R[f] - R_{emp}[f]) + R_{emp}[f^*] - R[f^*] \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If we assume that $\\sup_{f \\in \\mathcal{H}}(R[f] - R_{emp}[f]) \\xrightarrow[n \\rightarrow \\infty]{P} 0 $ (one sided uniform convergence over all functions in the hypothisis space) then:\n",
    "\n",
    "$$\n",
    "\\sup_{f \\in \\mathcal{H}}(R[f] - R_{emp}[f]) + R_{emp}[f^*] - R[f^*] \\xrightarrow[n \\rightarrow \\infty]{P} 0\n",
    "$$\n",
    "\n",
    "Which then means that by this assumption, this is a sufficient condition for consistency because the assumption implies the consistency of the ERM. \n",
    "Because by this assumption the terms all converge to zero which gives the consistency requirements stated above. By assumption we then have \n",
    "\n",
    "$$\n",
    "\\underbrace{R[f_n] - R[f^*]}_{\\text{1. Consistency}} + R_{emp}[f^*] - R_{emp}[f_n] \\xrightarrow[n \\rightarrow \\infty]{P} 0 \\quad \\text{by Assumption} \\\\\n",
    "$$\n",
    "Which means that the first term has to converge to zero.\n",
    "\n",
    "$$\n",
    "\\phantom{R[f_n]} - R[f^*] + R_{emp}[f^*]  \\phantom{-R_{emp}[f_n]} \\xrightarrow[n \\rightarrow \\infty]{P} 0 \\quad \\text{by Hoeffding's inequality}\\\\\n",
    "R[f_n] \\phantom{- R[f^*] + R_{emp}[f^*]} - R_{emp}[f_n] \\xrightarrow[n \\rightarrow \\infty]{P} 0 \\quad \\text{2. Consistency} \\\\\n",
    "$$\n",
    "\n",
    "Then because of our assumption, both consistency requirements are met for the ERM.\n",
    "\n",
    "Let $\\mathcal{H}$ be a set of functions with bounded loss for the distribution $F(x, y)$, $A \\leq R[f] \\leq B, \\ \\forall f \\in \\mathcal{H}$. \n",
    "For the ERM principle to be consistent, it is necessary and sufficient that \n",
    "$$ \n",
    "\\lim_{n \\rightarrow \\infty} \\mathbb{P}(\\sup_{f\\in \\mathcal{H}} (R[f] - R_{emp}[f]) > \\epsilon) = 0, \\ \\forall \\epsilon > 0\n",
    "$$\n"
   ],
   "id": "5f5ad1fe1884f2ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The key theorem of learning theory\n",
    "\n",
    "The key theorem asserts that any analysis of the convergence of ERM must be a worst case analysis. We now would like to look at simple hypothesis spaces and ask, are there simple hypothesis spaces for which consistency is guaranteed and what can we say about only finite sample sizes? \n",
    "\n",
    "Assume the set $ \\mathcal{H}$ contains only 2 functions.\n",
    "\n",
    "$$\n",
    "\\mathcal{H} = \\{f_1, f_2\\}\n",
    "$$\n",
    "\n",
    "Let \n",
    "\n",
    "$$\n",
    "C_{\\epsilon}^i := \\{(x_1, y_1), ..., (x_n, y_n)| R[f_i] - R_{emp}[f_i] > \\epsilon \\}\n",
    "$$\n",
    "\n",
    "be the set of samples for which the risks of $f_i$ differ by more than $\\epsilon$. By the Hoeffding's inequality we get\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(C_{\\epsilon}^i) \\leq \\exp(-2n\\epsilon^2)\n",
    "$$\n",
    "\n",
    "The union bound, i.e. the bound on the hypothesis space is given by\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\sup_{f \\in \\mathcal{H}}(R[f] - R_{emp}[f] > \\epsilon)) \n",
    "= \n",
    "\\mathbb{P}(C_{\\epsilon}^1 \\cup C_{\\epsilon}^2) \n",
    "= \n",
    "\\mathbb{P}(C_{\\epsilon}^1) + \\mathbb{P}(C_{\\epsilon}^2) - \\mathbb{P}(C_{\\epsilon}^1 \\cap C_{\\epsilon}^2) \n",
    "\\leq\n",
    "\\mathbb{P}(C_{\\epsilon}^1) + \\mathbb{P}(C_{\\epsilon}^2) \n",
    "\\leq\n",
    "2 \\exp(-2n\\epsilon^2)\n",
    "$$\n",
    "\n",
    "Assuming then now that $ \\mathcal{H}$ contains a finite number of functions $ \\mathcal{H} = \\{ f_1, ..., f_N \\}$.\n",
    "Then again by the union bound we get\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\cup_{i=1}^N C_{\\epsilon}^i) \\leq \\sum_{i=1}^N \\mathbb{P}(C_{\\epsilon}^i) \\leq N \\exp(-2n\\epsilon^2)\n",
    "$$\n",
    "\n",
    "Which then gives\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\sup_{f \\in \\mathcal{H}}(R[f] - R_{emp}[f] > \\epsilon)) \\leq N \\exp(-2n\\epsilon^2) = \\exp(ln N - 2n\\epsilon^2)\n",
    "$$\n",
    "\n",
    "Which means that for a finit hypothesis space the ERM is consistent. \n",
    "Thus we can bound the test error for the function which minimizes the emperical risk.\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(R[f_n] - R_{emp}[f_n] > \\epsilon) \\leq \\exp(ln N - 2n\\epsilon^2)\n",
    "$$\n",
    "\n",
    "We can then derive a confidence interval\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(R[f_n] - R_{enp}[f_n] > \\epsilon) \\leq \\exp(ln N - 2n\\epsilon^2) =: \\delta(\\epsilon) \\\\\n",
    "\\mathbb{P}(R[f_n] - R_{enp}[f_n] \\leq \\epsilon) \\geq 1 - \\delta(\\epsilon)\n",
    "$$\n",
    "\n",
    "With probability at least $1 - \\delta$ it holds that\n",
    "\n",
    "$$\n",
    "R[f_n] \\leq R_{emp}[f_n] + \\epsilon(\\delta) \\Rightarrow R[f_n] \\leq R_{emp}[f_n] + \\sqrt{\\frac{a}{n} \\left( ln N + ln \\frac{b}{\\delta} \\right) }\n",
    "$$\n",
    "\n",
    "with $a = 1/2$ and  $b = 1$.\n",
    "\n",
    "$$\n",
    "\\exp \\left( ln N - 2n\\epsilon^2 \\right) = \\delta \\Rightarrow-2n\\epsilon^2 =  ln \\delta - ln N \\Rightarrow \\epsilon = \\sqrt{\\frac{1}{2n} \\left( ln N - ln \\delta \\right)} = \\sqrt{\\frac{1}{2n} \\left( ln N + ln \\frac{1}{\\delta} \\right)}\n",
    "$$\n",
    "\n",
    "Here then $ln N = \\text{Capacity}(\\mathcal{H})$. The bound on the risk then only depdends on $ \\mathcal{H}$ and $n$."
   ],
   "id": "358ffa61f28178d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Infinite\n",
    "\n",
    "We note that $R_{emp}[f]$ refers only to a finite class, that is for $n$ sample points $x_1, ..., x_n$, the functions $f \\in \\mathcal{H}$ can take at most $2^n$ different values $y_1, ..., y_n$.\n",
    "Thus eventhough our space $ \\mathcal{H}$ is infinite, because we only have $n$ sample points, many functions give the same values $y_1, ..., y_n$.\n",
    "\n",
    "## Shattering Coefficient\n",
    "\n",
    "Let a sample $Z_n := \\{(x_1, y_1), ..., (x_n, y_n) \\}$ be given. Denote by $ \\mathcal{N}(\\mathcal{H}, Z_n) $ the cardinality of $\\mathcal{H}$ when restricted to $\\{x_1, ..., x_n \\}$, $\\mathcal{H}|Z_n$, i.e. the number of functions from $ \\mathcal{H} $ that can be distinguished on the given sample. The shattering Coefficient is the maximum number of ways into which $n$ point can be classified bny the function class \n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\mathcal{H}, n) = \\max_{Z_n} \\mathcal{N}(\\mathcal{H}, Z_n)\n",
    "$$\n",
    "\n",
    "Since $f(x) \\in  \\{-1, 1\\}, \\mathcal{N}(\\mathcal{H}, n)$ is finite.\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\mathcal{H}, Z_n) \\leq \\mathcal{N}(\\mathcal{H}, n) \\leq 2^n\n",
    "$$\n",
    "\n",
    "Because we're in the infinite case we want to find another way to express our capacity measure $ \\mathcal{H} $.\n",
    "Because $ \\mathcal{N}(\\mathcal{H}, Z_n) \\leq \\mathcal{N}(\\mathcal{H}, n)$, depends on the sample itself ($Z_n$) and $\\mathcal{N}(\\mathcal{H}, n) \\leq 2^n$ is a too loose bound we want a more strict bound.\n",
    "The dependency on the sample can be removed by averaging over all samples $ \\mathbb{E}\\left[ \\mathcal{N}(\\mathcal{H}, Z_n) \\right]$.\n",
    "\n",
    "* ***Theorem (Vapnik and Chervonenkis)***\n",
    "*Let $Z_{2n} = ((x_1, y_1), ..., (x_{2n}, y_{2n})$ be a sample of size $2n$. For any $\\epsilon > 0$ it holds that*\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\sup_{f \\in \\mathcal{H}}(R[f] - R_{emp}[f]) > \\epsilon) \\leq 4 \\exp \\left( ln \\mathbb{E}[\\mathcal{N}(\\mathcal{H}, Z_{2n}) ] - \\frac{n \\epsilon^2}{8} \\right)\n",
    "$$\n",
    "\n",
    "If then $ ln \\mathbb{E}[ \\mathcal{N}(\\mathcal{H}, Z_{2n})]$ grows sublinearly, we get a nontrivial bound."
   ],
   "id": "7e040459be729cf9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
