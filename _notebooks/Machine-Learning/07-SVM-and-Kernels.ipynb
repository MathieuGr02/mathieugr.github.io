{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SVM\n",
    "\n",
    "When the training examples are linearly seperable we can maximize the margin by minimizing the regularization term\n",
    "\n",
    "$$\n",
    " \\min_{\\mathbf{w}} \\frac{1}{2} ||\\mathbf{w}||^2 = \\frac{1}{2} \\sum_{i=1}^d w^2_i\n",
    "$$\n",
    "\n",
    "subject to the classification constraint (gives us a constraint which requires that we correctly classify our labels and that there is a minimum margin between the closest points and the hyperplane).\n",
    "\n",
    "$$\n",
    "s.t. \\quad y_i [\\mathbf{x}_i^T \\mathbf{w}] - 1 \\geq 0, i=1,...,n\n",
    "$$\n",
    "\n",
    "The solution is defined only on the basis of a subset of examples or support vectors ($y_i [\\mathbf{x}^T_i \\mathbf{w}] - 1 = 0$). \n",
    "The support vectors are the datapoints $ \\mathbf{x}_i $ which lie on the margin. \n",
    "The margin is thus defined by these support vectors.\n",
    "\n",
    "When the training examples are not lineary seperable we add a penalty for violating the classification constraint\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}} \\frac{1}{2} ||\\mathbf{w}||^2 + C\\sum_{i=1}^n \\xi_i\n",
    "$$\n",
    "\n",
    "subject to the relaxed constraint\n",
    "\n",
    "$$\n",
    "s.t. \\quad y_i [\\mathbf{x}^T_i \\mathbf{w}] - 1 + \\xi_i \\geq 0, i=1,...,n\n",
    "$$\n",
    "\n",
    "The $\\xi_i$ are called slack variables and it works as a shift variable which would shift the point back to the correct side of the margin.\n",
    "\n",
    "We can rewrite the non-seperable case as\n",
    "\n",
    "$$\n",
    "C \\sum_{i=1}^n (1 - y_i[\\mathbf{x}_i^T \\mathbf{w}])^+ + \\frac{1}{2} ||\\mathbf{w}||^2\n",
    "$$\n",
    "\n",
    "where $z^+ = t$ if $z \\geq 0$ else $0$. \n",
    "This is equivalent to reguralized emperical loss minimization (ridge regression).\n",
    "\n",
    "$$\n",
    "\\underbrace{\\frac{1}{n} \\sum_{i=1}^n (1 - y_i [\\mathbf{x}_i^T \\mathbf{w}])^+}_{R_{emp} \\ (\\text{Loss function})} + \\lambda ||\\mathbf{w}||^2, \\quad \\lambda = \\frac{1}{2nC}\n",
    "$$\n",
    "\n",
    "The SVM is also very similair to the Logistic Regression (LOGREG).\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{SVM} &: \\frac{1}{n} \\sum_{i=1}^n (1 - y_i [\\mathbf{x}_i^T \\mathbf{w}])^+ + \\lambda ||\\mathbf{w}||^2 \\\\\n",
    "    \\text{LOGREG} &: \\frac{1}{n} \\sum_{i=1}^n - \\log \\underbrace{\\sigma(y_i [\\mathbf{x}_i^T \\mathbf{w}])}_{\\mathbb{P}(y_i | \\mathbf{x}_i, \\mathbf{w})} + \\lambda ||\\mathbf{w}||^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $\\sigma(z) = (1 + e^{-z})^{-1}$ is the logistic function.\n",
    "\n",
    "The way they differ is in the usage of the loss function. \n",
    "While the SVM uses the loss $(1 - z)^+$ (hinge loss), the LOGREG uses the loss $\\log(1 + \\exp(-z))$."
   ],
   "id": "2952d39186e7eb90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Solution to SVM\n",
    "\n",
    "We want to solve\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}} \\ \\frac{1}{2} ||\\mathbf{w}||^2 \\quad s.t. \\quad y_i[\\mathbf{x}_i^T \\mathbf{w}] - 1 \\geq 0, i = 1, ..., n\n",
    "$$\n",
    "\n",
    "We rewrite our constraint with the help of the lagrangian multiplier into\n",
    "\n",
    "$$\n",
    "\\sup_{\\alpha_i \\geq 0} \\alpha_i (1 - y_i [\\mathbf{x}_i^T \\mathbf{w}]) = \n",
    "\\begin{cases}\n",
    "    0, \\quad if \\ y_i [\\mathbf{x}_i^T \\mathbf{w}] - 1 \\geq 0 \\\\\n",
    "    \\infty, \\quad otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Thus with lagrangian our minimization problem as a lagrangian function we get\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}} \\frac{1}{2} ||\\mathbf{w}||^2 + \\sum_{i=1}^n \\sup_{\\alpha_i \\geq 0} \\alpha_i(1 - y_i[\\mathbf{x}^T_i \\mathbf{w}]) \\\\\n",
    "\\Rightarrow \n",
    "\\min_{\\mathbf{w}} \\sup_{\\alpha_i \\geq 0} \\left( \\frac{1}{2} ||\\mathbf{w}||^2 + \\sum_{i=1}^n \\alpha_i(1 - y_i[\\mathbf{x}^T_i \\mathbf{w}]) \\right)\n",
    "$$\n",
    "\n",
    "We can swap the min and max problem by the use of slaters condition\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha \\geq 0} \\left( \\min_{\\mathbf{w}} \\left( \\frac{1}{2} ||\\mathbf{w}||^2 + \\sum_{i=1}^n \\alpha_i(1 - y_i[\\mathbf{x}^T_i \\mathbf{w}]) \\right) \\right)\n",
    "$$\n",
    "\n",
    "Because now the inner term (:= $ J(\\mathbf{w})$) is a convex function we can take it's derivative and set it to zero, which then gives us\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} = \\mathbf{w} - \\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i \\overset{!}{=} 0 \\\\\n",
    "\\Rightarrow \\mathbf{\\hat{w}} =  \\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "Plugging this back in to our initial problem gives us \n",
    "\n",
    "$$\n",
    "\\max_{\\alpha_i \\geq 0} \\left( \\frac{1}{2} ||\\hat{\\mathbf{w}}||^2 + \\sum_{i=1}^n \\alpha_i(1 - y_i[\\mathbf{x}^T_i \\hat{\\mathbf{w}}]) \\right) \\\\\n",
    "\\max_{\\alpha_i \\geq 0} \\left( \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n y_i y_j \\alpha_i \\alpha_j \\mathbf{x}_i^T \\mathbf{x}_j^T \\right) \\\\\n",
    "$$\n",
    "\n",
    "Here the function in terms of $\\alpha$ is a concave function therefor we can again easily calculate the optimal parameters by taking the derivative and setting it to zero. \n",
    "This maximization is the so called dual. \n",
    "Only $\\hat{\\alpha}_i$'s corresponding to the support vectors will be non-zero.\n",
    "\n",
    "If we now would like to make new predictions we do\n",
    "\n",
    "$$\n",
    "sign(\\mathbf{x}^T \\hat{\\mathbf{w}}) = sign(\\mathbf{x}^T \\sum_{i=1}^n \\hat{\\alpha}_i y_i \\mathbf{x}_i) = sign(\\sum_{i \\in SV} \\hat{\\alpha}_i y_i \\mathbf{x}_i^T \\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "The value of the function then on the input vectors only via the dot-product of the new datapoint vector and all the support vectors."
   ],
   "id": "882f067717c9dd2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Kernel function\n",
    "\n",
    "A kernel function is a real-valued function of two arguments, $k(\\mathbf{x}, \\mathbf{x}') \\in \\mathbb{R}$ for $ \\mathbf{x}, \\mathbf{x}' \\in \\mathcal{X} $, where $ \\mathcal{X}$ is the input space."
   ],
   "id": "a81d8ee4ef4ae4d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
