{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "title: Mixture Models\n",
    "layout: collection\n",
    "permalink: /Machine-Learning/Mixture-Models\n",
    "collection: Machine-Learning\n",
    "entries_layout: grid\n",
    "mathjax: true\n",
    "toc: true\n",
    "categories:\n",
    "  - study\n",
    "tags:\n",
    "  - mathematics\n",
    "  - statistics\n",
    "  - machine-learning \n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83ad73fd05231b76"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_theme()"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-22T11:22:58.002587Z",
     "start_time": "2024-05-22T11:22:57.988750Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "In mixture models we assume that the input examples come from different potentially unobserved types (groups, clusters etc.).\n",
    "We assume that there are $m$ underlying types where each type $z$ occurs with a certain probability $ \\mathbb{P}(z) $.\n",
    "The examples of the type $z$ are then conditionally distributed $ \\mathbb{P}(\\mathbf{x}|z )  $. \n",
    "The observations $ \\mathbf{x}   $ then come from a so called mixture distribution, which is just the weighted sum of the type probability times the conditional type probability\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\mathbf{x} ) = \\sum_{j=1}^m  \\mathbb{P}(z=j) \\mathbb{P}(\\mathbf{x} | z=j, \\mathbf{\\theta}_j )   \n",
    "$$\n",
    "\n",
    "A mixture of gaussians model has the form\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\mathbf{x} | \\mathbf{\\theta}  ) = \\sum_{j=1}^m \\pi_j \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_j, \\Sigma_j  )    \n",
    "$$\n",
    "\n",
    "where $ \\mathbf{\\theta} = \\pi_1, ..., \\pi_m | \\mathbf{\\mu}_1, ..., \\mathbf{\\mu}_m | \\Sigma_1, ..., \\Sigma_m   $. $\\pi_j$is the so called mixing proportion which can be seen as the probability of an observations coming from a class $j$. Thus the probability of a class itself.\n",
    "\n",
    "# Data generation\n",
    "\n",
    "During the data generation, with probability $ \\mathbb{P}(z)  $ class $z_j$ is chosen and the sample points $ \\mathbf{x} $ are chosen from the conditional distribution $ \\mathbb{P}(\\mathbf{x} | z = j )  $.\n",
    "For a two class system, our sample points $ \\mathbf{x}  $ could then have been generated in two ways. We thus would like to find out the underlying distribution of our observations.\n",
    "\n",
    "# Latent Variable Models (LVM).\n",
    "\n",
    "In the model $ \\mathbb{P} $(\\mathbf{x} | z=j, \\mathbf{\\theta}) the class indicator variable $z$ is latent. This means that $z$ is a variable that can only be indirectly inferred through mathematical models from other observable variables that can directly be observed. \n",
    "They are not directly measurable.\n",
    "This then is an example of a large calss of latent variable models (LVM)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38af7ed5f1f4bcd0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
