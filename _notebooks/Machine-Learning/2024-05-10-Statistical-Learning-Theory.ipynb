{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "title: Statistical Learning Theory\n",
    "layout: collection\n",
    "permalink: /Machine-Learning/Statistical-Learning-Theory\n",
    "collection: Machine-Learning\n",
    "entries_layout: grid\n",
    "mathjax: true\n",
    "toc: true\n",
    "categories:\n",
    "  - study\n",
    "tags:\n",
    "  - mathematics\n",
    "  - statistics\n",
    "  - machine-learning \n",
    "---"
   ],
   "id": "1803b6d0f8e72066"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Frequentist Decision theory\n",
    "\n",
    "Let $\\mathbf{x}$ be an i.i.d generated sample from a unknown pdf $\\mathbb{P}^*(\\mathbf{x})$ and the output values $y$ be from the unknown conditional pdf $\\mathbb{P}^*(y | \\mathbf{x})$.\n",
    "The language model is then trained on a set of observed pairs drawn from the true unknown pdf given by $\\mathbb{P}^*(\\mathbf{x}, y)$.\n",
    "With this we get our data $\\mathcal{D}=\\{(\\mathbf{x}_1, y_1),...\\} \\overset{i.i.d}{\\sim} \\mathbb{P}^*(\\mathbf{x}, y)$. \n",
    "We assume here that $\\mathbf{x}$ is observable, but $y$ isn't. \n",
    "We then consider an estimator $\\delta(\\mathcal{D})$, which is defined as a prediction function $\\hat{y} = f_{\\mathcal{D}}(\\mathbf{x})$.\n",
    "Since the data is random, so is the estimator, meaning the estimator is a RV. \n",
    "We can thus compate the estimates class membership with the true label via a loss function $L(y, f(\\mathbf{x})$.\n",
    "The expected risk is then the average loss with respect to the true unknown joint distribution $\\mathbb{P}^*(\\mathbf{x}, y)$, or it can be thought of as expectable error. \n",
    "\n",
    "$$\n",
    "R(f, \\mathbb{P}^*) \n",
    "= \n",
    "\\mathbb{E}_{\\mathbb{P}^*(\\mathbf{x}, y)}L(y, f(\\mathbf{x}) \n",
    "= \n",
    "\\mathbb{E}_{\\mathbb{P}^*(\\mathbf{x})} \\mathbb{E}_{\\mathbb{P}^*(y | \\mathbf{x})} L(y, f(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Because the expected risk itself depends on the unknown distribution $\\mathbb{P}^*$, and we don't know the true nature of $\\mathbb{P}^*$ we cannot calculate this risk directly. We can approximate it with the so called emperical distribution for the n observed samples in $\\mathcal{D}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}_{\\mathcal{D}}(\\mathbf{x}, y | \\mathcal{D}) = \\frac{1}{n} \\sum_{(\\mathbf{x}_i, y_i) \\in \\mathcal{D}} \\delta({\\mathbf{x} - \\mathbf{x}_i) \\delta(y,y_i)\n",
    "$$\n",
    "\n",
    "The emperical risk, which is the sample average of the loss is then given by \n",
    "\n",
    "$$\n",
    "R_{emp}(f | \\mathcal{D}) = \\frac{1}{n} \\sum_{i=1}^n L(y_i, f(\\mathbf{x}_i))\n",
    "$$"
   ],
   "id": "7e2c4c3df2cee458"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hypothesis space\n",
    "\n",
    "The best possible risk is defined as $\\text{inf}_f R[f]$. Because we often have to restrict our function space, thus our hypothesis space $\\mathcal{H}$, which is only a subset of the true space. \n",
    "In our given hypothesis space we define $f^*$ as the best possible function that can be implemented by a machine.\n",
    "\n",
    "$$\n",
    "f^* = \\arg \\min_{f \\in  \\mathcal{H}}\n",
    "$$\n",
    "\n",
    "We also denote $f_{\\mathcal{D}} \\in \\mathcal{H}$ as the empirical risk minimizer on a sample $\\mathcal{D}$\n",
    "\n",
    "$$\n",
    "f_n = f_{\\mathcal{D}} = \\arg \\min_{f \\in \\mathcal{H}} R_{emp}(f | \\mathcal{D})\n",
    "$$"
   ],
   "id": "73020ff43ccce45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Convergence\n",
    "\n",
    "SLT gives then results for bounding the error on the unseen data, given the training data. Thus a relation between the past ($\\mathcal{D}$) and the future (unseen data). In SLT all samples are i.i.d. We can bound the risk which with a probability of $1 - \\delta$ holds with $a, b > 0$:\n",
    "\n",
    "$$\n",
    "\n",
    "$R[f_n]$ \\leq R_{emp} + \\sqrt{\\frac{a}{n}\\left( \\text{capacity}(\\mathcal{H}) + \\ln \\frac{b}{\\delta} \\right)}\n",
    "\n",
    "$$\n",
    "\n",
    "Ont he covnerge of RV themselves, we say that X_n converges in probability to the random variable X as $n \\rightarrow \\infty$, iff, for all $\\epsilon > 0$, \n",
    "\n",
    "$$\n",
    "\\mathbb{P}(|X_n - X| > \\epsilon) \\rightarrow 0, \\text{as } n \\rightarrow \\infty\n",
    "$$\n",
    "\n",
    "We define this as $X_n \\overset{p}{\\rightarrow} X$ as $n \\rightarrow \\infty$."
   ],
   "id": "8f22fef979486751"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Binary classification\n",
    "\n",
    "In general here we only look at the binary case where $f : X \\rightarrow \\{-1, 1\\}$ with $L(y, f(\\mathbf{x}) = \\frac{1}{2}|1 - f(\\mathbf{x})y|$. \n",
    "The hypothisis space is then $\\mathcal{H}' = \\{f' = \\text{sign}(f) | f \\in \\mathcal{H}\\}$"
   ],
   "id": "7636ceda9404b107"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Consistency of Emperical risk minimizations (ERM)\n",
    "\n",
    "The principle of ERM is consistent if for any $\\epsilon > 0$, \n",
    "\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|R[f_n] - R[f^*]| > \\epsilon) = 0\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|R_{emp}[f_n] - R[f_n]| > \\epsilon) = 0\n",
    "$$\n",
    "\n",
    "Meaning that for $n \\rightarrow \\infty$ for the first case, the probability fo the true risk $R[f_n]$ deviating from the best possbile risk R[f^*] in our hypothisis space becomes zero, which means that the true risk of $f_n$ converges to the risk of the best possible function. The seconds case describes that the emperical risk, which is estimated from the data converges to it's true risk. \n",
    "\n",
    "![consistency erm](consistency_ERM.png)\n",
    "\n",
    "Only the condition $\\mathbb{P}(|R_{emp}[f] - R[f^*]|)$ does not suffice as a condition for consistency."
   ],
   "id": "730dace92a05e44b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hoeffding's inequality\n",
    "\n",
    "With Hoeffding's inequality we are able to bound our probability.\n",
    "Let $\\xi_i , i \\in [0, n]$ be independent instances of a bounded RV $\\xi$, with values in $[a, b]$. Denote their average by $ Q_n = \\frac{1}{n} \\sum_{i} \\xi_i $. Then for any $ \\epsilon > 0 $ we get:\n",
    "\n",
    "$$\n",
    "\\begin{rcases}\n",
    "\\mathbb{P}(Q_n - \\mathbb{E}(\\xi) \\geq \\epsilon) \\\\\n",
    "\\mathbb{P}(\\mathbb{E}(\\xi) - Q_n \\geq \\epsilon)\n",
    "\\end{rcases}\n",
    "\\leq \\exp \\left( -\\frac{2n\\epsilon^2}{(b - a)^2} \\right)\n",
    "$$  \n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\mathbb{P}(|Q_n - \\mathbb{E}(\\xi)| \\geq \\epsilon) \\leq 2 \\exp \\left( - \\frac{2n\\epsilon^2}{(b - a)^2} \\right)\n",
    "$$\n",
    "\n",
    "Looking at our binary classification, we define $\\xi$ to be a 0/1 loss function\n",
    "\n",
    "$$\n",
    "\\xi = \\frac{1}{2}|1 - f(\\mathbf{x})y| = L(y, f(\\mathbf{x}))\n",
    "$$\n",
    "\n",
    "Then we get that $ Q_n[f] = \\frac{1}{n} \\sum_{i=1}^{n} \\xi_i = \\frac{1}{n} \\sum_{i=1}^{n} L(y_i, f(\\mathbf{x}_i)) = R_{emp}[f] $ and $ \\mathbb{E}[\\xi] = \\mathbb{E}[L(y, f(\\mathbf{x}))] = R[f] $.\n",
    "becuase $\\xi_i$ are independent instances of a bounden RV $\\xi$ with values in [0, 1] we get\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(|R_{emp}[f] - R[f]| > \\epsilon) \\leq 2 \\exp \\left( -2n\\epsilon^2 \\right)\n",
    "$$\n",
    "\n",
    "With this, the hoeffding's inequality gives us a rate of convergence for any fixed function. \n",
    "This doesn't tell us that $ \\mathbb{P}(|R_{emp}[f_n] - R[f_n]| > \\epsilon) \\leq 2\\exp \\left(- 2n\\epsilon^2 \\right) $, because $f_n$ is not a fixed function. $f_n$ depends on the data $ \\mathcal{D} $. \n",
    "Because $f_n$ is chosen to minimize the emperical risk, it may change with inceasing n so it is not a fixed function, therefor the hoeffding's inequality cannot be applied to this convergence.\n",
    "For each fixed function $f$, we get $R_{emp}[f] \\xrightarrow[n \\rightarrow \\infty]{P} R[f]$, meaning that the emperical risk converges to the true expected risk for a function as $n \\rightarrow \\infty$.\n",
    "\n",
    "### Conditions for consistency\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f_n &:= \\arg \\min_{f \\in \\mathcal{H}} R_{emp}[f] \\\\\n",
    "    f^* &:= \\arg \\min_{f \\in \\mathcal{H}} R[f] \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\begin{align*}\n",
    "    R[f] - R[f^*] &\\geq 0, \\ \\forall f \\in \\mathcal{H} \\\\\n",
    "    R_{emp}[f] - R_{emp}[f_n] &\\geq 0, \\ \\forall f \\in \\mathcal{H}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For first case, because $f^*$ is the function that minimizes the expected risk for all functions in the hypothisis space, any function in the hypothisis space has an equal or higher risk compared to $f^*$. \n",
    "For the second case, because $f_n$ is the function that minimizes the emperical risk for all functions in the hypothisis space, any function in the hypothisis space has an equal or higher emperical risk compared to $f_n$. \n",
    "Because this holds for any function $f \\in \\mathcal{H}$, we set $f = f_n$ for the first case and $f = f^*$ for the second one.\n",
    "$$\n",
    "\\begin{align*}\n",
    "    R[f_n] - R[f^*] &\\geq 0 \\\\\n",
    "    R_{emp}[f^*] - R_{emp}[f_n] &\\geq 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can then write\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    0 \n",
    "    &\\leq \n",
    "    R[f_n] - R[f^*] + R_{emp}[f^*] - R_{emp}[f_n] \\\\\n",
    "    &=\n",
    "    R[f_n] - R_{emp}[f_n] + R_{emp}[f^*] - R[f^*] \\\\\n",
    "    & \\leq\n",
    "    \\sup_{f \\in \\mathcal{H}}(R[f] - R_{emp}[f]) + R_{emp}[f^*] - R[f^*] \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If we assume that $\\sup_{f \\in \\mathcal{H}}(R[f] - R_{emp}[f]) \\xrightarrow[n \\rightarrow \\infty]{P} 0 $ (one sided uniform convergence over all functions in the hypothisis space) then:\n",
    "\n",
    "$$\n",
    "\\sup_{f \\in \\mathcal{H}}(R[f] - R_{emp}[f]) + R_{emp}[f^*] - R[f^*] \\xrightarrow[n \\rightarrow \\infty]{P} 0\n",
    "$$\n",
    "\n",
    "Which then means that by this assumption, this is a sufficient condition for consistency because the assumption implies the consistency of the ERM.\n",
    "\n",
    "Let $\\mathcal{H}$ be a set of functions wih bounded loss for the distribution $F(x, y)$, $A \\leq R[f] \\leq B, \\ \\forall f \\in \\mathcal{H}$. \n",
    "For the ERM principle to be consistent, it is necessary and sufficient that \n",
    "$$ \n",
    "\\lim_{n \\rightarrow \\infty} \\mathbb{P}(\\sup_{f\\in \\mathcal{H}} (R[f] - R_{emp}[f]) > \\epsilon) = 0, \\ \\forall \\epsilon > 0\n",
    "$$\n"
   ],
   "id": "5f5ad1fe1884f2ce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
