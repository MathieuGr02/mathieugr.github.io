{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Markov Chains\n",
    "\n",
    "A random process is a process in which the trajectory of the system is not precisely determined and it is described in terms of probability distributions.\n",
    "\n",
    "Discrete random process is a process in which the system changes between discrete states. The parameters of these transitions are called transition probabilities.\n",
    "\n",
    "Markov property: The probability distribution over states at the next time step depends only on the current state of the system\n",
    "\n",
    "$$\n",
    "Pr(X_{n+1} = x | X_1 = x_1, X_2 = x_2, ...) = Pr(X_{n + 1} = x | X_n = x_n)\n",
    "$$\n",
    "\n",
    "A markov chain is a discrete random process with the markov property. In general, a $m$'th order markov considers the last $m$ steps."
   ],
   "id": "c3bee63f3e741f08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Estimating nucleotide probabilities\n",
    "\n",
    "Given a genome, we let $(\\pi_A, \\pi_C, \\pi_G, \\pi_T)$  be the probabilities of choosing each of the four nucleotides. To estimate these probabilities, we write the likelihood of the sequence given the model and find the parameters which maximize the likelihood. We look here at a 0-order markov chain.\n",
    "The probability is given as\n",
    "\n",
    "$$\n",
    "P(s | \\vec \\pi) = \\pi_A^{n_A} \\pi_C^{n_C} \\pi_G^{n_G} \\pi_T^{n_T}\n",
    "$$\n",
    "\n",
    "where $n_i$ is the amount of occurences of base $i$. The log likelihood is then\n",
    "\n",
    "$$\n",
    "L(s | \\vec \\pi) = n_A\\log(\\pi_A) + n_C\\log(\\pi_C) + n_G\\log(\\pi_G) + n_T\\log(\\pi_T)\n",
    "$$\n",
    "\n",
    "under the constraint that $\\pi_A + \\pi_C + \\pi_G + \\pi_T = 1$. This is solved with the lagrange multiplier where we want to maximize the function $f(x)$ s.t. the constraint $g(x) = c$. We define the function $\\Lambda(x, \\lambda) = f(x) + \\lambda(g(x) - c)$ and solve for the partial derivatives of $\\Lambda$ being 0.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\Lambda(\\vec \\pi | \\lambda) }{\\partial \\pi_{\\alpha}}\n",
    "    &= \n",
    "    \\frac{\\partial }{\\partial \\pi_{\\alpha}} ( n_A\\log(\\pi_A) + n_C\\log(\\pi_C) + n_G\\log(\\pi_G) + n_T\\log(\\pi_T) + \\lambda (\\pi_A + \\pi_C + \\pi_G + \\pi_T - 1)) \\\\\n",
    "    &=\n",
    "    \\frac{n_{\\alpha}}{\\pi_{\\alpha}} + \\lambda \\\\\n",
    "    &= 0 \\\\\n",
    "    \\Rightarrow - \\lambda &= \\frac{n_{\\alpha}}{\\pi_{\\alpha}}\n",
    "\\end{align*}\n",
    "\n",
    "Now because we have that\n",
    "\n",
    "$$\n",
    "\\sum_{\\alpha} \\pi_{\\alpha} = \\sum_{\\alpha} - \\frac{n_{\\alpha}}{\\lambda} = 1 \\Rightarrow n_A + n_C + n_G + n_T = - \\lambda\n",
    "$$\n",
    "\n",
    "Thus we have that the maximum likelihood solution is given as $ \\pi_{\\alpha} = \\frac{n_{\\alpha}}{n} $. \n",
    "\n",
    "\n",
    "Now looking at a 1-order markov chain, where the nucleotide at position $i$ depends only on the nucleotide at position $i - 1$. The probability of the sequence is then given as\n",
    "\n",
    "$$\n",
    "P(x) = P(x_L|x_{L - 1}) P(x_{L - 1} | x_{L - 2}) ... P(x_2 | x_1 ) P(x_1)  \n",
    "$$\n",
    "\n",
    "Similarly to the single nucleotide probability, we can estimate from the maximum likelihood the double nucleotide probabilities.\n",
    "\n",
    "| From\\To | A        | C        | G        | T        | Total   |\n",
    "|---------|----------|----------|----------|----------|---------|\n",
    "| A       | $n_{}$   | $n_{AC}$ | $n_{AG}$ | $n_{AT}$ | $n_{A}$ | \n",
    "| C       | $n_{CA}$ | $n_{CC}$ | $n_{CG}$ | $n_{CT}$ | $n_{C}$ | \n",
    "| G       | $n_{GA}$ | $n_{GC}$ | $n_{GG}$ | $n_{GT}$ | $n_{G}$ | \n",
    "| T       | $n_{TA}$ | $n_{TC}$ | $n_{TG}$ | $n_{TT}$ | $n_{T}$ | \n",
    "\n",
    "These frequencies are given as $\\pi_{\\alpha\\beta} = \\frac{n_{\\alpha \\beta}}{n_{\\alpha}} $ with $n_{\\alpha} = \\sum_{\\beta} n_{\\alpha \\beta}$."
   ],
   "id": "58868b7862d44283"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Detection of CpG islands\n",
    "\n",
    "Given a genome, the log odds ratio is given as \n",
    "\n",
    "$$\n",
    "S(x) = \\log \\left( \\frac{P(\\vec | CpG \\ model)}{P(\\vec | non- CpG \\ model)} \\right) = \\log(P(\\vec x | CpG \\ model)) - \\log(P(\\vec x | non-CpG \\ model))\n",
    "$$\n",
    "\n",
    "The Log-probability then of a sequence under the CpG island is given as \n",
    "\n",
    "$$\n",
    "\\sum \\log(p_{CpG}(\\alpha | \\beta))\n",
    "$$\n",
    "\n",
    "The Log-probability then of a sequence under the non CpG island is given as \n",
    "\n",
    "$$\n",
    "\\sum \\log(p_{\\overline{CpG}}(\\alpha | \\beta))\n",
    "$$\n",
    "\n",
    "Given precalculated data for the probabilities, we can use a sliding window over the genome sequence to see where the CpG islands are located. The problem with this is, how do we choose the size of the sliding window and at which probability is the cut-off range for a CpG island."
   ],
   "id": "ae4abed53c023da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hidden Markov Model\n",
    "\n",
    "In a hidden markov model, we model simultaneously the states (CpG island and non-CpG island) and the nucleotides that are observed in these states. The path ($\\pi$) between states is modeled as a markov chain and transitions in and out of the CpG island state specify the CpG island boundaries.\n",
    "\n",
    "This model consists of multiple parameters\n",
    "\n",
    "- State-to-State transition probabilities: Probability to go from state $k$ to state $l$ $$a_{kl} = P(\\pi_i = l | \\pi_{i-1} = k)$$\n",
    "- Emission Probabilities: Having at position $i$ the base $\\beta$ $$e_l(\\beta) = P(x_i = \\beta | \\pi_i = l, x_{i-1} = \\alpha)$$\n",
    " \n",
    "Probability to go from state $k$ to state $l$ and to emit letter $\\beta$ is then given as \n",
    "\n",
    "$$\n",
    "e_l(\\beta) a_{kl}\n",
    "$$\n",
    "\n",
    "The most likely path through the model given a sequence $x$ corresponds to the most likely assignment of the CpG islands through the sequence."
   ],
   "id": "af1195f505cb8477"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The Viterbi Algorithm\n",
    "\n",
    "The viterbi algorithm allow us to find the maximum likelihood of the path given our hidden markov model through recursion. We define $\\nu_k(i)$ as the maximum probability that we can achieve after observing the first $i$ letters of the sequence, ending in state $k$. Through recursion we can then find the best path\n",
    "\n",
    "$$\n",
    "\\nu_k(i) = e_k(x_i) \\max_{l} (\\nu_l (i - 1) a_{lk})\n",
    "$$ \n",
    "\n",
    "During this recursion, we keep track of the sequence of states $k$ that gave us the maximum probability at each nucleotide $i$ in the sequence. \n",
    "\n",
    "*Initialization*\n",
    "- $\\nu_S(\\phi) = 1, \\nu_k(\\phi) = 0$ $\\forall k$ state other than start\n",
    "\n",
    "*Recursion*\n",
    "- $\\nu_k(i) = e_k(x_i) \\max_l (\\nu_l(i - 1) a_{lk})$\n",
    "- $ptr_i(k) = \\operatorname{argmax}_l (\\nu_l(i - 1) a_{lk})$\n",
    "\n",
    "*Termination*\n",
    "- $P(\\vec x, \\pi^*) = \\max_k (\\nu_k (L) a_{kE})$\n",
    "- $\\pi^*_L = \\operatorname{argmax}_k (\\nu_k(L) a_{kE})$\n",
    "\n",
    "*Traceback*\n",
    "- $\\pi_{i-1}^* = ptr_i(\\pi^*_i) $ $\\forall i = L...1$"
   ],
   "id": "cf7d90920bbfe8a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Forward Algorithm\n",
    "\n",
    "Due to the optimal path being only one of many paths, with varying amounts of similarities between one another. To find more \"stable\" CpG islands, i.e. islands that occur often in high probability paths, we look at the posterior probabilities if the paths.\n",
    "\n",
    "The forward algorithm gives us the probability of a sequence taking into account that multiple paths can give rise to the same sequence of symbols. The total probability is given as\n",
    "\n",
    "$$\n",
    "P(\\vec x) = \\sum_{\\pi} P(\\vec x, \\pi)\n",
    "$$\n",
    "\n",
    "This allows us to calculate the probabilities of individual paths as \n",
    "\n",
    "$$\n",
    "P(\\pi | \\vec x) = \\frac{P(\\vec x, \\pi)}{P(\\vec x)} = \\frac{P(\\vec x, \\pi)}{\\sum_{\\pi'} P(\\vec x, \\pi')}\n",
    "$$\n",
    "\n",
    "Here in the forward algorithm, we calculate the probability to end in state $k$ after reading the first $i$ symbols\n",
    "\n",
    "$$\n",
    "f_k(i) = P(x_1, ..., x_i, \\pi_i = k)\n",
    "$$\n",
    "\n",
    "*Initialization*\n",
    "- $f_S(\\phi) = 1, f_k(\\phi) = 0$ $\\forall k$ state other than start\n",
    "\n",
    "*Recursion*\n",
    "- $f_k(i) = e_k(x_i) \\underbrace{\\sum_l (f_l(i-1) a_{lk})}_{\\text{All previous paths transitioning to state } k}$\n",
    "\n",
    "*Termination*\n",
    "- $P(\\vec x) = \\sum_k (f_k(L) a_{kE})$"
   ],
   "id": "d6f5016005784e89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Backward Algorithm\n",
    "\n",
    "The backward algorithm gives us the probability of a sequence taking into account that multiple paths can give rise to the same sequence of symbols, but doing the computations from the end towards the beginning from the sequence. We calculate the probability of symbols $x_{i + 1}, ... , x_L$ starting in state $k$ at position $i$.\n",
    "\n",
    "$$\n",
    "b_k(i) = P(x_{i+1}, ..., x_L, \\pi_i = k)\n",
    "$$\n",
    "\n",
    "*Initialization*\n",
    "- $b_k(L) = a_{kE}$ $\\forall k$\n",
    "\n",
    "*Recursion*\n",
    "- $b_k(i) = \\sum_{l} b_l(i+1)e_l(x_{i+1})a_{kl}$\n",
    "\n",
    "*Termination*\n",
    "- $b_s(\\phi) = P(x_1,...,x_L | \\pi_0 = S) = P(\\vec x) = \\sum_k b_k(1) e_k(x_1)a_{Sk}$"
   ],
   "id": "67536e9a0ebc32e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Most probable State\n",
    "\n",
    "The forward backward prob can be seen as the total probability from paths, where position $i$ is in state $k$. The forward, as the name suggest looks at all the paths from $0$ to $i$ and the backwards looks at all the paths from $L$ to $i$.\n",
    "\n",
    "Having the forward and backwards probabilities, we can calculate the probablitity that position $i$ is in a CpG island\n",
    "\n",
    "$$\n",
    "P(\\vec x, \\pi_i = k) = \\underbrace{P(x_1, ..., x_i, \\pi_i = k)}_{f_k(i)} \\underbrace{P(x_{i+1}, ..., x_L, \\pi_i = k)}_{b_i(k)}\n",
    "$$\n",
    "\n",
    "It follows that then\n",
    "\n",
    "$$\n",
    "P(\\pi_i = k | \\vec x) = \\frac{P(\\vec x, \\pi_i = k)}{P(\\vec x)} = \\frac{f_k(i)b_k(i)}{P(\\vec x)}\n",
    "$$"
   ],
   "id": "2129decb093068f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Distribution of state lengths\n",
    "\n",
    "We would like to know the average length of a CpG island. This can be estimated by\n",
    "\n",
    "$$\n",
    "P(l) = a_{CC}^l (1 - a_{CC})\n",
    "$$\n",
    "\n",
    "It follows then that\n",
    "\n",
    "\\begin{align*}\n",
    "    \\langle l \\rangle\n",
    "    &=\n",
    "    \\sum_{l=0}^{\\infty} l P(l) \\\\\n",
    "    &=\n",
    "    \\sum_{l=0}^{\\infty} l a_{CC}^l (1 - a_{CC}) \\\\\n",
    "    &=\n",
    "    a_{CC}(1 - a_{CC})\\sum_{l=0}^{\\infty} l a_{CC}^{l - 1} \\\\\n",
    "    &=\n",
    "    a_{CC}(1 - a_{CC}) \\frac{\\partial}{\\partial a_{CC}} \\sum_{l=0}^{\\infty} a_{CC}^{l} \\\\\n",
    "    &=\n",
    "    a_{CC}(1 - a_{CC}) \\frac{\\partial}{\\partial a_{CC}} \\frac{1}{1 - a_{CC}} \\\\\n",
    "    &=\n",
    "    a_{CC}(1 - a_{CC}) \\frac{1}{(1 - a_{CC})^2} \\\\\n",
    "    &=\n",
    "    \\frac{a_{CC}}{1 - a_{CC}} \\\\\n",
    "\\end{align*}"
   ],
   "id": "b3e4940015cd25b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The problem with this model setup is that we assumed we know the transition and emission probabilities, which in actuality are unknown. When the state sequence is known, we simply need to compute the number of transitions and emissions to calculate\n",
    "\n",
    "$$\n",
    "a_{kl} = \\frac{A_{kl}}{\\sum_{l'} A_{kl'}} \\qquad e_{k} (\\beta) = \\frac{E_{k}(\\beta)}{\\sum_{\\beta'}E_k(\\beta')}\n",
    "$$\n",
    "\n",
    "Due to the state sequence being unknown, we use the expectation maximization to optimize these parameters."
   ],
   "id": "b4eeb1fa5bfb246"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Baum-Welch Algorithm\n",
    "\n",
    "Based on the current values of $a_{kl}$ and $e_k(\\beta)$, compute the expected values of $A_{kl}$ and $E_k(\\beta)$, considering probable paths. We then re-estimate $a_{kl}$ and $e_k(\\beta)$.\n",
    "The posterior probability that transition $a_{kl}$ is used at position $i$ in sequence $\\vec x$ is given as \n",
    "\n",
    "$$\n",
    "P(\\pi_i = k, \\pi_{i+1} = l | \\vec x, \\theta) = \\frac{f_k(i)a_{kl}e_l(x_{i+1})b_l(i+1)}{P(\\vec x)}\n",
    "$$\n",
    "\n",
    "By then summing over all training sequences $j$ and all positions $i$, we get \n",
    "\n",
    "\\begin{align*}\n",
    "    A_{kl} &= \\sum_j \\frac{1}{P(\\vec x^j)} \\sum_i f_k^j(i) a_{kl} e_l(x_{i+1}^j) b_l^j(i+1) \n",
    "    E_k(\\beta) &= \\sum_j \\frac{1}{P(\\vec x^j)} \\sum_{\\{i | x_i^j = \\beta\\}} f_k^j(i) b_k^j(i)\n",
    "\\end{align*}\n",
    "\n",
    "and then we recalculate \n",
    "\n",
    "$$\n",
    "a_{kl} = \\frac{A_{kl}}{\\sum_{l'} A_{kl'}} \\qquad e_{k} (\\beta) = \\frac{E_{k}(\\beta)}{\\sum_{\\beta'}E_k(\\beta')}\n",
    "$$\n",
    "\n"
   ],
   "id": "b5459c8663094da0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b42f32a97cff16a0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
