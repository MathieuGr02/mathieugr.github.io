{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "title: Maximal Entropy\n",
    "layout: collection\n",
    "permalink: /Computational-Biology/Maximal-Entropy\n",
    "collection: Computational-Biology\n",
    "mathjax: true\n",
    "toc: true\n",
    "categories:\n",
    "  - study\n",
    "tags:\n",
    "  - mathematics\n",
    "  - statistics\n",
    "---"
   ],
   "id": "5e5b02100882fe03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Laplace's Method\n",
    "\n",
    "Assume we are estimating a single parameter $\\mu$, and given our data $ \\mathcal{D}$ we hav calculated a posterior distribution $ \\mathbb{P}(\\mu | \\mathcal{D})$. Because we may want to integrate the posterior over some area, because the posterior can be a complicated function to integrate, if the posterior has a single peak, we can estimate it by expanding the logarithm of the posterior around the peak.\n",
    "\n",
    "Let $\\mu_*$ be the mode of the distribution, thus the peak of $ \\mathbb{P}(\\mu | \\mathcal{D})$. \n",
    "We definde $L(\\mu) = \\log \\mathbb{P}(\\mu |\\mathcal{D)$. Expanding this around $\\mu_*$ with the taylor expansion gives us \n",
    "\n",
    "$$\n",
    "L(\\mu) \n",
    "\\approx \n",
    "L(\\mu_*) + \n",
    "\\frac{\\partial}{\\partial \\mu} L(\\mu) \\Bigr|_{\\mu = \\mu_*} (\\mu - \\mu_*)+\n",
    "\\frac{1}{2} \\frac{\\partial^2}{\\partial \\mu^2} L(\\mu) \\Bigr|_{\\mu = \\mu_*} (\\mu - \\mu_*)^2 + ...\n",
    "$$\n",
    "We only take the terms up to the second derivative as a good enough approximation. Because then $\\frac{\\partial}{\\partial \\mu} L(\\mu) \\Bigr|_{\\mu = \\mu_*} = 0$, because at $\\mu_*$ the posterior has a maxima, we get \n",
    "\n",
    "$$\n",
    "L(\\mu) \n",
    "\\approx \n",
    "L(\\mu_*) + \\frac{1}{2} L''(\\mu_*) (\\mu - \\mu_*)^2\n",
    "$$\n",
    "\n",
    "Then setting $L''(\\mu_*) = \\frac{1}{\\sigma^2}$ and taking the exponent we get a gaussian:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\mu | \\mathcal{D}) = e^{L(\\mu)} \\approx e^{L(\\mu_*)} e^{ - \\frac{(\\mu - \\mu_*)}{2 \\sigma^2}}\n",
    "$$\n",
    "\n",
    "Assume now that we have n parameters $ \\mathbf{\\alpha} = (\\alpha_1, \\alpha_2, ..., \\alpha_n)$ and have a likelihood function $ \\mathbf{P}(\\mathcal{D} | \\mathbf{\\alpha})$. \n",
    "Using a uniform prior we get the posterior $ \\mathbb{P}(\\mathbf{\\alpha} | \\mathcal{D}) \\propto \\mathbf{P}(\\mathcal{D} | \\mathbf{\\alpha})$. The value $ \\mathbf{\\alpha}^*$ then maximizes the posterior.\n",
    "\n",
    "Expanding then around the logarithm we get\n",
    "\n",
    "$$\n",
    "\\log \\mathbb{P}(\\mathbf{\\alpha} | \\mathcal{D}) \n",
    "= \n",
    "\\log \\mathbb{P}(\\mathbf{\\alpha}^* | \\mathcal{D}) +\n",
    "\\sum_{i} (\\alpha_i - \\alpha_i^*) \\frac{\\partial \\log \\mathbb{P}(\\mathbf{\\alpha} | \\mathcal{D})}{\\partial \\alpha_i } \\Bigr|_{\\mathbf{\\alpha} = \\mathbf{\\alpha}^*} + \n",
    "\\sum_{i,j} (\\alpha_i - \\alpha_i^*) (\\alpha_j - \\alpha_j^*)  \\frac{\\partial^2 \\log \\mathbb{P}(\\mathbf{\\alpha}^2 | \\mathcal{D})}{\\partial \\alpha_i } \\Bigr|_{\\mathbf{\\alpha} = \\mathbf{\\alpha}^*} + ...\n",
    "$$ \n",
    "\n",
    "Again the first term vanishes because of the maxima at $ \\mathbf{\\alpha}^*$. The second derivate gives the so called hessian matrix.\n",
    "\n",
    "$$\n",
    "H_{ij} = \\frac{\\partial \\log \\mathbb{P}(\\mathbf{\\alpha} | \\mathcal{D})}{\\partial \\alpha_i \\alpha_j} \\Bigr|_{\\mathbf{\\alpha}=\\mathbf{\\alpha}^*}\n",
    "$$\n",
    "\n",
    "If we set $B_{ij} = -H_{ij}$ we get the approximated posterior by a multivariate gaussian\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\mathbf{\\alpha} | \\mathcal{D}) \\propto \\exp \\left( - \\frac{1}{2} \\sum_{i,j}(\\alpha_i - \\alpha_i^*) B_{ij} (\\alpha_j - \\alpha_j^*) \\right)\n",
    "$$\n",
    "\n",
    "The covariance matrix is then given by the inverse of the hessian. If now integrate all parameters but one, the marginal distribution again is a gaussian with variance $\\sigma^2_i = B_{ij}^{-1}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\alpha_i | \\mathcal{D}) \\propto \\exp \\left( -\\frac{1}{2} \\frac{(\\alpha_i - \\alpha_i^*)^2}{B_{ij}^{-1} \\right)\n",
    "$$"
   ],
   "id": "5888ae09cc2c2092"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ----\n",
    "\n",
    "Assume we have $N$ genes in total, where each gene can have $m=0,1,2,...,$ mRNA copies in a given cell. \n",
    "On average, a gene has $\\bar{m}$ mRNAs per cell, i.e. there are $M=\\bar{m}N$ mRNAs per cell. \n",
    "We now would like to know, what is the probability $ \\mathbb{P}(m | I)$ that one particular gene has $m$ mRNAs in the cell.\n",
    "\n",
    "1. Standard way\n",
    "\n",
    "For each gene $g$ we specify the number $m_g$ of mRNAs that it has in the cell, i.e. the mutually exclusive and exhaustive possibilities are vectors $ \\mathbf{m} = (m_1, m_2, ... , m_N)$. We can then incorporate our prior information that among these potentially possible states, our information $I$ specifies that only those states have nonzero probability for which we have:\n",
    "\n",
    "$$\n",
    "M(\\mathbf{m}) = \\sum_{g=1}^{N} m_g = M = N \\bar{m}\n",
    "$$\n",
    "\n",
    "This we have a uniform prior which restricts our space to the space which has the previous given property\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\mathbf{m} | I) = \\frac{\\delta(M (\\mathbf{m}) - N \\bar{m})}{\\sum_{\\mathbf{m}'} \\delta(M (\\mathbf{m}') - N \\bar{m})}\n",
    "$$\n",
    "\n",
    "and for a single gene we have\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(m_g = m | I) = \\frac{\\sum_{\\mathbf{m}} \\delta(M(\\mathbf{m}) - N \\bar{m}) \\delta(m_g - m))}{\\sum_{\\mathbf{m}'} \\delta(M (\\mathbf{m}') - N \\bar{m})}\n",
    "$$\n",
    "\n",
    "For large N these calculations become difficult for large N, which is why we look at an approximate.\n",
    "Instead of looking at th number of mRNAs $m_g$ that each gene $g$ has, we look at the ector with the number of genes that have precisely $m$ mRNA copies $ \\mathbf{n} = (n_0, n_1, n_2, ..., n_M)$, which counts how many genes there are with 0 mRNA ($n_0$), with 1 mRNA ($n_1$) etc. \n",
    "For a given vector $ \\mathbf{n}$ there are many vectors $ \\mathbf{m}$. \n",
    "Let then $W(\\mathbf{n})$ denote the number of vectors $ \\mathbf{m}$ that all havce the same count vector $ \\mathbf{n}$.\n",
    "The numbers  $W(\\mathbf{n})$ are given by the multinomial coefficients.\n",
    "\n",
    "$$\n",
    "W(\\mathbf{n}) = \\frac{N!}{n_0 ! n_1 ! ... n_M!}\n",
    "$$\n",
    "With our constraint $M( \\mathbf{n}) = \\sum_{m=1}^{\\infty} mn_m = M = N \\bar{m}$.Then our probability distribution becomes\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\mathbf{n} | I) = \\frac{W(\\mathbf{n}) \\delta(M(\\mathbf{n}) - N\\bar{m})}{\\sum_{\\mathbf{n}'} W(\\mathbf{n}')\\delta(M(\\mathbf{n'}) - N\\bar{m})}\n",
    "$$"
   ],
   "id": "d944ac4e3dd2c398"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
